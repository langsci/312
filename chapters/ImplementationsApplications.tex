\documentclass[output=paper,hidelinks]{langscibook}
\ChapterDOI{10.5281/zenodo.10185986}
\title{Computational implementations and applications}
\author{Martin Forst\affiliation{Cerence Inc.} and  Tracy Holloway King\affiliation{Adobe Inc.}}
\abstract{Computational implementations of LFG are computer programs composed of LFG annotated c-structure rules and lexical entries. LFG was designed to be computationally tractable and has a strong history of broad-coverage grammar implementations for diverse languages. As with theoretical LFG, implemented grammars primarily focus on c-structure and f-structure, but the resulting f-structures are used as input to semantics and abstract knowledge representation, and some work has focused on the integration of morphological and phonological information as well as argument structure. From a theoretical linguistic perspective, implemented grammars allow the linguist to test analyses and to see interactions between different parts of the grammar. From an application perspective, applications such as machine translation and question answering take advantage of the abstract f-structures and the ability of LFG grammars to parse and generate as well as to detect (un)grammaticality.}

\IfFileExists{../localcommands.tex}{
   \addbibresource{../localbibliography.bib}
   \addbibresource{thisvolume.bib}
   \input{../localpackages}
   \input{../localcommands}
   \input{../localhyphenation}
   \togglepaper[24]%%chapternumber
}{}

\begin{document}
\maketitle
\label{chap:ImplementationsApplications}

Computational implementations of LFG are computer programs composed of LFG annotated c-structure rules and lexical entries. When parsing, they take as input  a natural language sentence and output c-structures and f-structures and potentially other projections such as semantics. When generating, they take as input an f-structure and generate a grammatical natural language sentence. As with theoretical LFG, these implemented grammars obey the fundamental premises of LFG such as completeness, coherence, and uniqueness.\footnote{This contrasts with approaches which produce f-structure-like representations but do not use LFG principles or machinery. See \sectref{sec:pargram} and \cite{cahilletal02}.}\ LFG was designed from the outset to be computationally tractable and has a strong history of broad-coverage implementations for multiple languages, primarily through the ParGram project \citep{ButtEtAl1999} which is built on the XLE grammar development platform \citep{xledoc}. 

\hspace*{-.5pt}Grammar engineering involves the implementation of linguistically-motivated grammars so that natural language utterances and text can be processed to produce deep syntactic, and sometimes semantic, structures. As with theoretical LFG, implemented grammars primarily focus on c-struc\-ture and f-structure. The resulting f-structures have been used extensively as input to semantics and abstract knowledge representation. Other work has focused on the integration of morphological and phonological information, as well as argument structure, but in general these areas have lagged behind the proposals in the theoretical literature. In addition, implemented LFG grammars have been used to create large-scale tree and dependency banks, mapping a corpus of sentences to a set of f-structures or related dependency structures.

% THK: commented out since Emily is correct that the topic shift is abrupt; we talk about all this later
%The ParGram project is based on the theoretical LFG hypothesis that languages are more similar at f-structure, which encode grammatical functions, than at c-structure, which encodes word order and constituency. This f-structure similarity can  be exploited in applications such as machine  translation. Other applications which take advantage of the more abstract f-structures and the ability of LFG grammars to parse and generate as well as detect (un)grammaticality include computer-assisted language learning, question answering, and sentence condensation. From a theoretical linguistic perspective, implemented grammars allow the linguist to test analyses and to see interactions between different parts of the grammar. 

We first introduce the computational implementations of LFG, presenting specific platforms and touching upon aspects such as core components, grammar development tools, modularity, and runtime performance (\sectref{sec:compimpl}). We then discuss implications for theoretical issues (\sectref{sec:theoretical_implications}) and the ParGram grammar resources (\sectref{sec:pargram}). Finally, we outline existing and potential applications for LFG implemented grammars (\sectref{sec:applications}).


\section{Computational implementations}
\label{sec:compimpl}

Computational implementations of LFG grammars focus on annotated phrase structure rules and lexical entries.
% THK: commented out; for those outside LFG implementation, this is too abrupt; later we talk about the DCU style grammars in a reference
%Without the lexicon and  annotated phrase structure rules needed to produce c- and f-structures, an implementation is not an LFG grammar. 
These implementations concentrate on creating high-quality f-structures since most applications use f-structures as their input (\sectref{sec:applications}). This section first introduces the major platforms that support LFG implementations. The core components provided by these platforms are then outlined, followed by some specific grammar development tools. Finally two computational notions, modularity and performance, are discussed.

%\input{systems}

\subsection{Platforms}
\label{sec:systems}

Since the inception of LFG as a grammar framework several  platforms  aimed at processing text according to the LFG formalism have been created. These platforms range from an M.Sc.\ project \citep{Minos2014} and introductory French implementation \citep{zweigenbaum91} to an industrially funded grammar development and processing platform which was actively developed for over two decades: the Xerox Linguistic Environment (XLE). In between those in terms of breadth of applicability and technical maturity are systems developed in academic research institutions, in particular XLFG, SxLFG, and the Free Linguistic Environment (FLE). Active development on many of these systems is limited: for current status and documentation the platform owners should be consulted.

\subsubsection{XLFG and Elvex}

\noindent XLFG \citep{clementkinyon01} is a parsing platform that was first implemented for didactic purposes.\footnote{XLFG is available at \url{http://www.xlfg.org}}\ It has been used to verify the soundness of several proposals to handle a variety of linguistic phenomena (\sectref{sec:theoretical_implications}), e.g.\ zeugmas, particle verbs, and non-constituent coordination \citep{clement19}.

XLFG uses an Earley parser \citep{earley70} for context-free parsing, and then resolves the f-structure constraints on packed c-structure representations \citep{maxwellkaplan89,maxwellkaplan93}. It  expects tokenized sentences as input and uses full-form lexicons for lexical lookup (\sectref{sec:corecomponents}). XLFG does not facilitate the use of external components like finite-state transducers for preprocessing tasks such as tokenization or morphological analysis (\sectref{sec:corecomponents}). It has primarily been applied to parsing French and English, i.e.\ analyzing French or English text into f-structures. Recently, work was started on a generator, i.e.\ mapping f-structures to text, using XLFG-style grammars for the production of surface realizations from f-structures. This generator is named Elvex.\footnote{Elvex is available at \url{https://github.com/lionelclement/Elvex}}

\subsubsection{SxLFG}
        
SxLFG \citep{Boullier:2005:ERL:1654494.1654495} was also developed with the participation of Lionel Cl\'ement, but its main authors are Pierre Boullier and Beno\^{\i}t Sagot of INRIA. The primary focus of SxLFG is on the deep non-probabilistic parsing of large corpora \citep{sagot-boullier-2006-deep} by means of robustness techniques for input sentences for which no spanning c-structure can be produced. The underlying context-free parser is the Earley parser of the SYNTAX project. Like XLFG and XLE, SxLFG resolves f-structure constraints on packed c-structure representations. The French broad-coverage LFG implementation that has been used extensively with SxLFG includes a large full-form lexicon for French, the Lefff 2 \citep{sagotetal06}. Like XLFG, SxLFG does not facilitate the use of external components like finite-state transducers for preprocessing tasks such as tokenization or morphological analysis (\sectref{sec:corecomponents}). SxLFG was developed for parsing. Generation has not been in the scope of SxLFG.
 
\subsubsection{XLE (and GWW as precursor) and XLE-Web}\label{sec:ImpApp:XLE}

The Xerox Linguistic Environment (XLE) was developed by the Natural Language Theory and Technology (NLTT) group at the Xerox Palo Alto Research Center (PARC). It started as a reimplementation in C of the earlier Grammar Writer's Workbench (GWW) \citep{kaplanmaxwell96}, which was implemented in Lisp and is still available.  XLE was used by several academic and industry teams for the development of LFG implementations for more than a dozen languages (see \sectref{sec:pargram} on the ParGram project). XLE, in conjunction with a customized broad-coverage grammar, was used to parse the English Wikipedia in the Powerset search engine \citep{kaplan09}.

XLE has mostly been used for parsing, but it includes a generator that can efficiently produce surface realizations from f-structures and even packed f-structure charts \citep{Maxwell06}. Thanks to this bidirectionality, it has powered applications such as machine translation and sentence condensation (\sectref{sec:applications}), and it has been used in research projects on stochastic realization ranking \citep{CahillForst}.

From its inception, XLE was designed to use finite-state transducers for low-level processing steps such as (de)tokenization and morphological analysis and generation (\sectref{sec:corecomponents}). The interface can readily be used with transducers in Xerox's finite-state transducer format including ones converted from the Foma finite-state transducers, and with relatively little programming effort, other external components  can  be integrated into an XLE grammar (e.g.\ see \cite{Fang-King-GEAF07} on integrating a non-finite state Chinese word breaker into an XLE Chinese grammar).

In addition to the $\phi$-projection from c-structures to f-structures, the XLE parser supports further projections from either of those representations. One of them, the optimality  structure, is hard-coded to guide the parsing and generation process on the basis of optimality marks (\sectref{sec:runtime_performance}). The use of optimality marks as a robustness mechanism is one of
the many extensions of XLE born out of a joint effort of the group at PARC and its ParGram partners.

Other extensions of the parser and generator are aimed at reducing latency and at ranking the (top n) parses or realizations. For the former, the most notable mechanism is c-structure pruning \citep{cahill-etal-2008-speeding}. C-structure pruning relies on corpus data annotated with (partial) constituent bracketing and learns to eliminate highly unlikely c-structures before the computationally expensive resolution of f-annotations. For the latter, a component for training\footnote{Training data comprises sentences with labeled bracketing, which can be derived from treebanks or created manually \citep{Riezler2002King}.}\ and applying maximum-entropy models based on a large variety of features is provided as part of XLE \citep{Riezler2002King}.

Beyond the parser and the generator, XLE also contains a term-rewriting component which was first developed for transfer-based machine translation but has  been used for a number of other purposes: identifying and deleting modifiers in f-structures that can be deleted without changing the meaning of the corresponding sentences too much \citep{riezleretal03}; treebank (\citetv{chapters/Treebanks}) conversion from one dependency-oriented format into another \citep{forst-2003-treebank}; further normalization of f-structures and/or construction of semantic representations \citep{crouchking06,bobrowetal07}; extraction of features for parse ranking \citep{forst07} and realization ranking \citep{CahillForst}.

Currently XLE is used by the academic members of the ParGram initiative (\sectref{sec:pargram}) as well as by individual researchers. It can be used online with LFG implementations for a number of languages via XLE-Web,\footnote{XLE-Web is available at \url{http://clarino.uib.no/iness/xle-web}}\ a web interface for XLE developed at the University of Bergen, and is used as part of the INESS infrastructure developed there \citep{Rosen09, rosen-desmedt-meurer-dyvik12}. See \citetv{chapters/Treebanks} for details on using INESS for parsebanking and more generally the uses of LFG parsebanks.
% THK: commented out since Emily thought this was too much detail and I am sure it is covered in the treebank chapter that we cite
%ParseBanker-based efforts have given rise to syntactically analyzed corpora of considerable size in Norwegian \citep{dyvikEtAl2016,rosenetalccorpus05} and Polish \citep{patprz14}. 
XLE is available for non-commercial research purposes.\footnote{XLE is available at \url{https://ling.sprachwiss.uni-konstanz.de/pages/xle/redmine.html}}\ Uses beyond that require a license agreement with PARC and Xerox.

\subsubsection{FLE}

The Free Linguistic Environment (FLE) \citep{cavaretal16} aims to create an LFG-oriented grammar-development and parsing environment with a license less restrictive than XLE's. It is implemented in C++ and  uses the same grammar syntax as XLE, but it is subject to the Apache 2.0 license. In addition to the context-free grammar format of XLE, it supports two probabilistic context-free grammar formats. For tokenization and morphological analysis, FLE provides an interface to Foma transducers.\footnote{Foma supports the import from and the export to XFST formats and XFST supports Foma transducers.}\  FLE uses open-source components when possible. FLE provides basic parsing functionality but does not contain a generator capable of producing surface strings for input f-structures.\footnote{FLE is available at \url{https://gorilla.linguistlist.org/fle/}}

% \noindent  http://ioperm.org/lfg-parser.html

%\input{core_components}
\subsection{Core components}
\label{sec:corecomponents}
  
  The LFG systems described above allow grammar writers to implement LFG grammars with annotated phrase structure rules and lexical entries similar to those in theoretical LFG.   The %lexical entries and annotated phrase structure rules used in  implementations look like those in theoretical LFG, although 
  main difference is that the formatting is %often 
  specified with easier-to-type variants, e.g.\ symbols like $\uparrow$ and $\downarrow$ are replaced  with {\textasciicircum} and !.

\ea Example theoretical and implemented annotated c-structure rules:\\[1ex]
\begin{tabular}[t]{cc}
    \begin{tabular}{@{}ccc@{}}
    \multicolumn{3}{l}{Theoretical notation:}\\
        S $\longrightarrow$ & NP & VP  \\
         & ($\uparrow$ SUBJ)=$\downarrow$ & $\uparrow$=$\downarrow$\\
    \end{tabular}
    &
    \begin{tabular}{cl}
    \multicolumn{2}{l}{Implementation (XLE system) notation:}\\
        S -{}-$>$ &  NP:  ({\textasciicircum} SUBJ)=!;\\
         & VP: \/ {\textasciicircum}=!. \\
    \end{tabular}
    \end{tabular}
\z
           
\subsubsection{Preprocessing} 
In order to implement an LFG grammar, it is necessary to preprocess the text that the grammar will parse.  Minimally the preprocessing contains a tokenizer which breaks the text into tokens (i.e.\ words) and canonicalizes the capitalization if necessary (e.g.\ lowercasing sentence initial capitalized words in English unless they are proper nouns). These canonicalized tokens are then looked up in the lexicon. Implemented lexicons are similar to their theoretical counterparts, comprising the word, its part of speech, and f-structure annotations such as \textsc{pred}, \textsc{case}, and \textsc{number}. This information is integrated into the grammar via the annotated c-structure rules,  as in theoretical LFG. Many implementations  integrate a morphological analyzer which associates inflected forms of words with their lemma and morphological information. When using a morphological analyzer, the text is first tokenized and canonicalized for capitalization and then processed by the morphology. The output of the morphology (lemmata and morphological tags) are  looked up in the lexicon.  This simplifies the lexicon which only has to contain the lemmata and the morphological tags instead of containing all the inflected forms. These morphologies are often finite-state transducers (FSTs; \cite{beesleykarttunen03}) which can be used for both parsing and generation.\footnote{Parsing goes from a string (e.g.\ a natural language sentence) to a c- and f-structure. Generation goes from an f-structure to a natural language string. Most theoretical LFG focuses on parsing, although some accounts, especially OT-LFG ones (see papers in \cite{sells2001}), discuss generation.}\ For more details on using FSTs for preprocessing for LFG grammars see \cite{kaplanetal04} and \cite{boegelbuttking2019}, for integration of externally developed morphologies and lexicons within LFG grammars see \cite{kaplannewman97}.


\begin{figure}[ht]
\begin{tabular}{llll}
 Original text:&  \multicolumn{3}{l}{Dogs   barked. }\\
 Tokenization:   & Dogs & barked & .\\
 & dogs &  & \\
 Morphology: & dog +Noun +Pl  & bark +Verb +Past & . +Punct\\
 & dog +Verb +3Sg & &\\
\end{tabular}
\caption{Example preprocessing: Tokenization and morphological analysis}
\label{table:tokmorph}
\end{figure}

A given inflected form can have multiple morphological analyses. Often all the analyses are provided as input to the LFG grammar, and the c-structure rules and f-structure constraints are used to eliminate analyses which are not feasible in the context of the sentence (e.g.\ the verbal analysis of {\em dogs} in figure \ref{table:tokmorph}). Preprocessing with a part-of-speech (PoS) tagger  marks each word with its part of speech, as in (\ref{ex:postag}).   This information can be used to prune  the morphological analyses and thus constrain the c-structure built over the sentence. Since PoS taggers are not perfect even for well-edited  text, only certain tags are kept, or fall-back techniques are used when no analysis is found. See \cite{kaplanking03} and \cite{dalrymple06} for more details on integrating PoS taggers into LFG and other symbolic grammars.


\ea
  \label{ex:postag}  Dogs/Noun barked/Verb and/Conj the/Det cat/Noun left/Verb ./Punct
    
    \z

           
\subsubsection{Projections} Theoretical LFG posits  projections beyond the original lexicon, c-structure and f-structure. The exact number and combination of these projections is a subject of lively debate (\citetv{chapters/Intro}). These include Lexical Mapping Theory (LMT) to map between underlying argument structure and grammatical functions in the lexicon (\citetv{chapters/Mapping}), phonological and prosodic projections (\citetv{chapters/Prosody}), semantics and semantic structure (\citetv{chapters/Glue}), and information structure for discourse function information (\citetv{chapters/InformationStructure}). Most LFG implemented grammars do not include these additional projections because f-structures are sufficient for the applications they target. Even when other projections are included, they are often different from their theoretical counterparts both in their format and in how they are projected or derived. The primary additional component that is  included is the semantic component.  This component is based on the f-structure and is generally not a projection but instead is a separate post-processing step, although in some stages of its development the Norwegian ParGram grammar NorGram \citep{dyvikEtAl2016,dyvikEtAl2019} included a semantic projection \citep{halvorsen83,kaplan1987three,halvorsenKaplan95} whose representations were in Minimal Recursion Semantics \citep{copestakeetal05}. Semantic components include Glue semantics \citep{dalrympleetal93,messmerzylma18} and ordered rewriting rules \citep{crouchking06}. The ordered rewriting rules have been extended into abstract knowledge representations \citep{bobrowetal07}. XLE-based implementations have been created for morphological structure \citep{buttetal96} and prosodic structure  and information structure \citep{buttking98}, although none of these are used in  large-scale grammars. Instead, they focus on testing theoretical hypotheses and determining the complex interactions among different grammar components (\sectref{sec:theoretical_implications}). The lack of an implementation of LMT has resulted in issues for the parsing of morphologically rich languages like Turkish and Urdu, where interactions between 
passive and causative constructions cannot be easily captured in LFG implementations (\sectref{sec:theoretical_implications}; \cite{cetinogluetal08}). 

\subsubsection{Ambiguity} Implemented grammars often include components to handle ambiguity (see \sectref{sec:runtime_performance}). There are three broad areas around  managing ambiguity: computing all the analyses efficiently; representing the ambiguities compactly; resolving the ambiguity so that it does not need to be computed and represented. The first two are discussed in  \citetv{chapters/Computational} and \citetv{chapters/Treebanks}. Within the grammar writer's control are components including preprocessing by PoS taggers and named entity recognition systems, Optimality-Theory marks to prefer some constructions over others, and stochastic ranking of analyses.

\subsubsection{Configuration} The determination of which components (e.g.\ which tokenizer, morphology, lexicons, and annotated c-structure rules) to use in an implemented grammar need to be specified in a configuration (see \cite{xledoc} on how this is done in XLE). These may have default values, e.g.\ a tokenizer which simply splits sentences at spaces and does not deal with capitalization or punctuation, but large-scale grammars require customized components for the specific language and often the type of text (e.g.\ newspaper text, tweets). In addition, to allow for rapid extension to specific applications which may have new vocabulary and unusual constructions, these configurations allow the grammar writer to specify lexicons and rules that add to or override those in a standard base grammar \citep{kingmaxwell07}. For example, to parse English academic biology papers, special lexicons of biological terms as well as special c-structure rules for section titles might be added to a grammar of standard written English.

% \input{grammar_development_tools}
\subsection{Grammar development tools}
 \label{sec:devtools}
 
 To aid the grammar writer in managing a large-scale, broad-coverage LFG grammar, specialized variants of standard software development tools are needed. These grammar development tools are part of any LFG platform (\sectref{sec:systems}). Throughout this chapter we rely on examples from XLE \citep{xledoc}, which is the most broadly adopted LFG grammar development framework and is used in the ParGram project (\sectref{sec:pargram}).
 
\subsubsection{Grammar writer interface} Grammar-development tools for the creation of LFG implementations facilitate the creation of  c-structure rules and lexicon entries that are annotated with LFG functional annotations. Some platforms, e.g.\ Xerox's Grammar Writer's Workbench and XLFG, provide special interfaces for rules and lexicon entries. Others, e.g. XLE, use editors such as Emacs or the Eclipse-based eXLEpse \citep{Radle2011eXLEp-30056}.  The interfaces provide a way to apply the rules (i.e.\ the grammar) to a given input string and to output a c-structure and an f-structure graph in  human-readable and machine-readable formats. They also generally provide tools to help debug issues such as why a well-formed input sentence does not receive an analysis or why the analysis is incorrect.
            
\subsubsection{Macros and templates}  Since grammar engineers want to efficiently encode patterns across lexicon entries and grammar rules, some platforms support additional notations. XLE, for example, supports regular-expression macros that can expand to anything from a piece of f-annotation to an entire rule as well as f-annotation templates, e.g.\ to allow for like-category coordination over any c-structure category. Using a shared definition of templates across parallel LFG implementations for various languages and domains considerably facilitates the adherence to the agreed-upon f-structure conventions \citep{King2005}. For example, using a template \@\textsc{number} wherever number on nouns is assigned ensures that the same attribute (e.g.\ \textsc{numb}) is used and that it only needs to be changed in one place if later another name of the attribute is used (e.g.\ \textsc{num} instead of \textsc{numb}). See \sectref{sec:theoretical_implications} for discussion of the role of macros and templates in theoretical LFG.
        
\subsubsection{Feature table and feature space}  In a grammar formalism with untyped attri\-bute-value matrices such as LFG, it is not strictly necessary to declare the valid values for the attributes used in f-structures and potential other levels of representation. However, from an engineering standpoint, it is highly desirable to make sure that only valid values are used; this way, unintended deviations due to typos can be caught easily \citep{crouchking08}. This need to enforce the adherence to a set of conventions is heightened in efforts to develop parallel LFG implementations for various languages such as ParGram (\sectref{sec:pargram}). XLE therefore supports feature declarations which state all the features, i.e.\ attributes, and their values that are allowed in the grammar. Multiple feature declarations can be combined to check the  grammar code for adherence to them. In ParGram, each grammar combines the common feature declaration with a  language-specific one which adds additional language-specific features and declares which subset of values are allowed, e.g. for English the {\em dual}\/ value of the \textsc{num} attribute is removed.
        
\subsubsection{Treebanks as test suites} Treebanks, and more specifically f-structure banks (\citetv{chapters/Treebanks}), can be used as a form of detailed, LFG-specific test suite for the grammar's coverage. Creating the treebank highlights missing constructions and vocabulary in the grammar. The INESS-based Parsebanker (\citetv{chapters/Treebanks}) provides infrastructure for rapidly selecting the best parse from an XLE analysis by making use of c- and f-structure discriminants \citep{Rosen07lfg}. These discriminants are stored as part of the parsebanking to allow for rapid updating as the grammar evolves.  The grammar is then enhanced to account for these and the treebank is reparsed with the updated grammar and the new version of the treebank is inspected. This aids both in improving coverage and in ensuring that changes to the grammar do not break constructions that were previously covered. This approach has been used extensively in the development of the Norwegian \citep{dyvikEtAl2016}, Polish \citep{patprz12}, and Wolof \citep{Dione:Disambiguation} grammars.
        
\subsubsection{Version control} Version control is used in software development to track changes to the software being developed. As with software development more generally, version control in grammar development allows the grammar writer to compare two versions of a rule, lexical entry, or any other part of the grammar, to revert  to a previous version if needed, and to view conflicting changes. Version control systems also record who made a particular change, which makes it easier for multiple people to work on a grammar simultaneously by highlighting recent changes, especially conflicting ones. To our knowledge, eXLEpse \citep{Radle2011eXLEp-30056} is the only LFG-oriented editor that offers support for a variety of version-control systems. Since eXLEpse is based on Eclipse, all version-control plugins for Eclipse can be used. However, although XLE does not provide a version control system, most large scale grammars use a standard software version control system such as SVN or Git. In addition, regression testing by providing sentences and analyses known to be parsable by the grammar help in determining whether new versions of a grammar function properly \citep{chatzetal07,depaivaking08}.
        
\subsubsection{Documentation} As with any software development project, it is important to document what each part of the implemented grammar does. This takes the form of comments in the lexicon and annotated phrase structure rules, including examples of sentences which that part of the grammar can parse. \cite{dipper03} designed a self-documenting grammar system whereby the comments are extracted into proper, stand-alone documentation and example test suites of constructions covered by the grammar.
       

% \input{modularity_integration}
\subsection{Modularity and integration of  systems}


LFG is an inherently modular linguistic theory, with different representations and components for the lexicon, phrase (constituent) structure, functional structure, semantics, etc. This  is highlighted in implemented systems which introduce two other types of modularity: modularity for the grammar components, which correlates with the linguistic modularity, and modularity within those components, which enables better grammar engineering practices. LFG implementations are software systems and hence modularity of the different components is important for developing, scaling, maintaining and debugging the system.  This section describes how the modularity of the grammar components helps with grammar implementation.
       
       A core tenet of LFG is that different parts of the grammar require different types of representations. This is echoed in the implementations where the different modules can be created by different people and use different types of technology. As with theoretical LFG, the c-structure is a tree and the f-structure an attribute-value matrix, and the two are related via annotated phrase-structure rules. These phrase-structure rules form one module of the grammar. Similarly, lexicons comprise  word forms, parts-of-speech, and f-annotations. These form another module. These lexicons can be custom-created for the LFG grammar or converted from other lexical resources  \citep{kaplannewman97,sheilorsnes06,przepiorkowskietal14,patprz14}. The morphological component is often implemented as a finite-state transducer \citep{kaplanetal04,boegelbuttking2019} but can be of any form.\footnote{The non-FST morphologies are referred to as library transducers in XLE.}\ For example, the  ParGram Chinese grammar uses a combined tokenizer and part-of-speech tagger that was externally developed for non-LFG purposes \citep{Fang-King-GEAF07}. The importance of modularity is highlighted by the treatment of semantics: there have been many implementational approaches to semantic representations based on the LFG f-structure analyses. These include projecting the semantics as an attribute-value matrix \citep{halvorsen83,halvorsenKaplan95,asudeh2006direct,dyvikEtAl2016,dyvikEtAl2019}, implementing Glue Semantics \citep{dalrympleetal93,messmerzylma18}, and using ordered rewrite rules \citep{crouchking06}. Without a modular system, this exploration of the best way to capture the semantics would be difficult.
       
       There are three additional reasons to maintain modularity in an implemented grammar. The first is that large scale grammars often have multiple grammar writers. By having different files for the lexicon, templates, and annotated phrase structure rules, the efforts can be divided in such a way that changes can be easily merged. To further aid this, the lexicons and phrase-structure rules often comprise multiple files, e.g. the lexicon might be divided into verbs, closed-class items, and all other entries, and the phrase-structure rules might be divided into clausal and nominal. The second reason is that debugging, i.e. the process of finding and fixing errors in the grammar, is simpler in a more modular system. By having different components and different files within those components, the structure of the grammar is easier to see and the individual rules easier to locate. This debugging is further aided by the use of test suites \citep{chatzetal07,depaivaking08}, including ones based on examples in comments in the grammar rules \citep{dipper03}. Even with modularity, the inclusion of OT marks (\sectref{sec:runtime_performance}) can make debugging more complex since an analysis may not surface due to competition with another analysis. A third reason is that as described in \sectref{sec:corecomponents} and \sectref{sec:devtools}, in addition to a lexicon and annotated phrase structure rules,   LFG implementations can have tokenizers, morphologies, templates, feature tables, etc. These are combined via configuration files that encode the different modules of the system and the way they interact.

%\input{runtime_performance}

\subsection{Runtime performance}
\label{sec:runtime_performance}

When implemented grammars are used to test linguistic hypotheses and analyses (\sectref{sec:theoretical_implications}), how quickly the grammar provides an analysis for a sentence, i.e.\ its latency, is generally not important. However, almost all other uses for implemented grammars (\sectref{sec:applications}) have latency considerations. LFG implementations have provided a number of techniques to improve latency, sometimes at the cost of accuracy and coverage, e.g.\ certain analyses may be lost due to early elimination of possible structures \citep{kaplanetal04}.  There are two main issues with runtime performance of LFG grammars: ambiguity and latency. These considerations hold for both parsing and generation; we focus on parsing here.\footnote{See \citetv{chapters/Computational} on the inherent formal and computational properties of LFG.}

Ambiguity concerns the multiple analyses (i.e.\ c- and f-structures) that are assigned to a given sentence. The ambiguity problem is accentuated when there is no semantic or pragmatic processing to guide the choice among the different analyses. The ambiguities fall into three broad categories. First, sentences can have multiple analyses, all of which are correct and equally plausible out of context, e.g.\ in {\em I saw her duck} either I saw a bird or I saw a person ducking down. Second, sentences can have correct analyses but even out of context some of them are highly improbable, e.g.\ in {\em I saw the child with the telescope} there are two plausible readings where {\em saw} is the past tense of the verb {\em see} and one implausible one where {\em saw} is the present tense of the verb meaning to cut with a saw, which is only plausible in a bizarre magic show. Third, ambiguities can arise when the grammar allows ungrammatical analyses, either intentionally as a fall-back mechanism or unintentionally due to an error in the implementation. \cite{coppermansegond96} provide one of the first detailed expositions of ambiguity in LFG grammars, comparing the ambiguity discussed in the theoretical linguistics literature with that in implemented grammars. \cite{kingetal04} discuss ambiguity in LFG grammar writing in detail, focusing on the XLE-based LFG implementations.

Language contains ambiguities at many levels, from determining word boundaries in tokenization, to morphological analysis, to syntactic attachment ambiguities, to semantic quantifier scope and beyond. This can result in thousands of analyses even for short sentences and long processing times to compute each analysis. There are two main ways to handle this ambiguity efficiently. One is to handle the ambiguity by ``packing'' \citep{maxwellkaplan89,maxwellkaplan93,shemtov97} and operating at each level efficiently over the packed representations. Packing allows operations to apply just once to shared parts of the representation instead of enumerating all of the possibilities and processing each of them separately. For example, XLE is designed to maintain packed structures from the tokenization and morphology to the syntactic c- and f-structures and then into an ordered rule writing system that can be used to create semantic representations \citep{crouchking06}. The other way to handle ambiguity is to choose the most likely analysis at each level. For example, if there are multiple morphological analyses for a word (e.g.\ English {\em leaves}), the system can choose the most likely one given the information it has at that time (e.g.\ the words adjacent to {\em leaves} and their potential morphological analyses). This has the downside that the correct analysis may be lost due to removing information early \citep{dalrymple06}. 

 Optimality Theory (OT) (\citetv{chapters/OT}) can be used to allow the grammar writer to prefer certain analyses and even to control which grammar rules are active. \cite{franketal98,franketal01} propose an extension of the classical LFG projection architecture to incorporate a constraint ranking mechanism inspired by OT. A new projection, the o-projection, specifies violable constraints, which are used to determine a ``winner'' among competing, alternative analyses.  Many ambiguities can be filtered from the set of possible analyses for a given sentence by using this constraint ranking mechanism in the XLE system.  For example, OT marks can be used to prefer verbal analyses over adjectival ones in copular clauses with passives like {\em They were eaten}. XLE further provides a way to cut down the  search space in parsing, allowing for potentially fewer parses to search through. This is done via a special \textsc{stoppoint} feature, which is part of the Optimality Theory preference mechanism incorporated into XLE \citep{kingetal00}.  The OT marks can be grouped with certain groups only applying if no parse is found with the original set of OT marks.   That is, XLE will process the input in multiple passes, using larger and larger versions of the grammar in subsequent reparsing phases. These groupings are referred to as \textsc{stoppoint}s. \textsc{stoppoint}s are useful for eliminating ungrammatical analyses when grammatical analyses are present and for speeding up the parser by only using expensive and rare constructions when no other analysis is available.  If a solution can be found with the smaller, restricted grammar, XLE will terminate with this solution. Otherwise, a reparsing phase is triggered. This approach can be used to prefer multi-word expressions, for instance so that XLE will only consider analyses that involve the individual components of the multi-word expression if there is no valid analysis involving the multi-word expression. In addition to the OT marks, c-structure pruning \citep{cahill-etal-2008-speeding} and part-of-speech tagging and named entity recognition \citep{kaplanking03,dalrymple06,kkpat15} can be used to eliminate unlikely c-structures before unification.
         
Even with the use of OT marks, a sentence may have many valid parses. However, downstream applications often expect a single analysis, i.e.\ a single f-struc\-ture, as input. To use LFG grammars as input to such applications, statistical methods can be used to choose the most probable analysis \citep{Riezler2002King}. These stochastic models are trained on treebanks or dependency banks of known correct analyses. As a variant of this, \cite{dalrymple06} and \cite{kkpat15} investigated using a stochastic part-of-speech tagger to trim potential analyses before constructing the c- and f-structure.


%\input{theoretical_implications}
\section{Implications for theoretical issues}
\label{sec:theoretical_implications}

LFG and HPSG (\citealt{pollard1994head-driven} and, for an implementational perspective, \citealt{benderemerson19}) are in the privileged position of having not only a community of theoretical linguists but also of grammar engineers, with significant crossover between the theoretical and grammar-engineering communities.  There are four areas in which grammar engineering interacts with theoretical linguistics \citep{king2011,king16}.  These include: using grammar engineering to confirm linguistic hypotheses; linguistic issues highlighted by grammar engineering; implementation capabilities guiding theoretical analyses; and insights into architecture issues.   The positive feedback loop between theoretical and implementational efforts is a domain in which LFG and HPSG have a distinct advantage compared to many other linguistic theories, given the strong communities and resources available.

\subsection{Confirming linguistic hypotheses} Grammar engineering can be used to confirm linguistic hypotheses \citep{Bierwisch:63,Mueller99,Buttetal:99,Bender:08,Ben:Fli:Oep:11,king2011,Fokkens:14,king16,MuellerCoreGram}. Encoding the hypothesis in an implemented grammar not only highlights details of the analysis that might be missed in a pencil-and-paper version but can also bring to light interesting interactions with other linguistic phenomena, especially when the hypothesized analysis is encoded in a broad-coverage grammar. Two examples of this type include the analysis of determiner agreement systems and the prosody-syntax interaction.

\cite{kingdalrymple04} provide an LFG analysis of determiner agreement and noun conjunction, looking particularly at indeterminacy of agreement features. In order to test the proposed system, they implemented a toy grammar with lexical entries of each type and enough syntactic structure to encompass determiner, adjective, and verb agreement with conjoined and non-conjoined nouns. As a result, the authors were able to confirm that their analysis was formally sound and accounted for the known data. This toy grammar was relatively easy to implement in XLE because all of the necessary components, e.g.\ distributive features, were already available. 

Implementing proposals for the prosody-syntax interaction in LFG is more challenging because not all of the mechanisms that have been proposed in the literature are available in systems like XLE. \cite{buttking98} used an existing, non-LFG analysis of Bengali clitics and implemented it in order to test whether p(rosodic)-structure could be used to capture the generalizations proposed in the theoretical analysis, focusing on where mismatches between prosodic and syntactic structure occur. A much different interface approach was pursued in \cite{boegeletal09}, which built upon the finite-state transducers used for tokenization and morphological analysis within the grammars (\sectref{sec:corecomponents}). Finally, a large-scale implementation of certain phonology-syntax interactions was completed for Welsh \citep{mittendorfsadler06}. 
 
 \subsection{Implementational devices}\label{sec:ImpApp:Devices}
  
 Writing large-scale grammars highlights the interaction of different parts of the grammar and the need to be able to formally state certain types of generalizations. These needs have led to the creation of formal devices, some of which have become part of theoretical LFG analyses while others remain implementational devices.  Implementation capabilities that guided theoretical analysis include the use of complex categories for auxiliary analysis in English and German, the analysis of Welsh phonology-syntax interactions through the interaction of morphological analysis via finite-state transducers and the LFG c-structure, and the introduction of templates and macros.\largerpage
 
 Complex categories \citep{xledoc} are a formal c-structure device. They allow for generalizations over c-structure categories by having the category be composed of a fixed component and a variable, where the variable can pass its value to other complex categories on the right-hand side of the rule. In this way, they allow the grammar writer to capture generalizations through notation. This notation is then automatically compiled into standard c-structure rules. Complex categories are used to constrain the order and form of auxiliaries and main verbs in English (e.g.\ {\em They will have been promoted.}) by having each auxiliary state its meaning and its form (e.g.\ {\em have} is an AUX[perf,base] with perfective meaning and base form while {\em been} is an AUX[pass,perf] with passive meaning and pefective form) and the VP rules themselves are complex categories that reflect their head and based on that put requirements on their complement. 
 
 Welsh consonant mutations are a phenomenon whereby the initial consonant of certain words changes based on  its phonological and syntactic environment (\citetv{chapters/Celtic}). To capture the joint requirements on the mor\-pho\-phon\-o\-logy and the syntax which trigger mutations, \cite{mittendorfsadler06} used the finite-state morphology capabilities integrated in XLE to control where Welsh consonant mutations occur by encoding the boundary conditions in the morphological tag sequences. The modular nature of LFG combined with the implementational device of finite-state morphology provided a clean solution to the different types of triggers for the mutations.
 
 A long standing debate in the linguistic literature, especially for constraint-based formalisms like HPSG and LFG, is whether a comprehensive and efficient grammatical theory should include a type hierarchy and what role it should play. Historically HPSG has had types as foundational to the theory  while LFG has not. However, in grammar engineering, it is important to be able to efficiently capture generalizations as well as exceptions to those generalizations. The introduction of templates into the formal devices available to LFG allows for generalizations and inheritance via notation, without introducing a full type hierarchy into the formalism \citep{dalrymple2004linguistic,crouchking08} and as a result, the concept of templates has become part of theoretical LFG analyses. Similar to complex categories, templates and macros allow the grammar writer to capture generalizations through notation, which is then automatically compiled into standard LFG c- and f-structure rules.
 
 Two more minor formal devices which are gaining traction in  theoretical analyses are instantiation and local variables (a third is the restriction operator discussed in the next section). Since the beginning, predicates (\textsc{pred}) in LFG have not been unifiable with one another due to their unique lexical index \citep{kaplanbresnan82}. Certain non-\textsc{pred} features also need to be non-unifiable \citep{dalrymple01}. This can be captured by instantiation, represented by having the value of the feature be followed by an underscore. For example, instantiating the form values of English particles blocks their occurring multiple times in a sentence (e.g.\ *{\em they threw out the garbage out}) (see Figure~\ref{fig:mt} for an English example and \cite{forstetal10}).  Finally, local variables anchor a functional uncertainty equation to a particular f-structure and then refer to that f-structure in other annotations \citep{dalrymple01,xledoc}. This is needed when making a set of statements about a particular element of a set or a particular type of governing element. For example \cite{Szucs2019} uses local variables to state constraints on topic left dislocation constructions in Hungarian.
   
\subsection{Architectural issues}

Implementing a wide variety of phenomena, as is necessary for broad-coverage grammars,  brings to light architectural issues with the theory.  \cite{cetinogluetal08} and \cite{boegelbuttking2019} describe issues with the interaction of the passive and causative in Turkish and Urdu. These issues are the result of how lexical rules in LFG interact with complex predicate formation, where the passive is traditionally analyzed as involving a lexical rule while the causative is often analyzed as a complex predicate.  The Urdu and Turkish grammars use the restriction operator \citep{kaplanwedekind93} in the annotated c-structure rules to model complex predication, including causatives. The restriction operator allows for features of f-structures to be restricted out, i.e.\ to cause the grammar to function as if these features did not exist. This allows  complex predicate-argument structures to be built dynamically \citep{buttetal03,buttetal09}. In contrast, the passive is handled by lexical rules which apply to the predication frames in the lexicon. This predicts that passivization applies before causativization and that it is not possible to passivize a causative by demoting or suppressing the subject of the causative. However, this is the reverse of the Urdu and Turkish facts. To solve this problem in the ParGram grammars of Urdu and Turkish, both the causative and the passive are handled via restriction in the annotated phrase structure rules. In the theoretical literature, this issue had not been highlighted because for Turkish and Urdu style morphosyntax, the causative was handled in argument-structure, but the interaction between causativization and passives at the morphology-syntax interface highlighted that traditional lexical rules do not allow for the right order of application when causativization is morphological but passivization is part of the syntax.
    
To conclude this section, the interaction of grammar engineering and theoretical linguistics helps to confirm linguistic hypotheses, to highlight complex linguistic issues, to posit new formal capabilities, and shed light on  architecture issues.   The positive feedback loop between theoretical and implementational efforts is a domain in which LFG and HPSG have a distinct advantage.


\section{Grammar resources: ParGram}

%\input{pargram}
\label{sec:pargram}

The systems described above are used to create small- and large-scale LFG grammars. These can be used as input to applications (\sectref{sec:applications}) or to explore theoretical hypotheses (\sectref{sec:theoretical_implications}). The Parallel Grammar (ParGram) project is a consortium of LFG researchers implementing grammars for a typologically varied set of languages in a parallel fashion \citep{ButtEtAl1999,Butt02KingMasuichiRohrer} using the XLE LFG parser, generator, and grammar development platform. The parallels are most notable in the f-structure space, where common features and analyses are used wherever possible, but differ when required by the syntax of the languages. This parallelism is enabled by LFG theory, by grammar engineering components such as feature declarations, and by semi-annual meetings between the grammar writers.\footnote{A similar approach was subsequently adopted by the HPSG DELPH-IN consortium \citep{benderetal02}.}

ParGram  began with three languages: English \citep{Riezler2002King}, French \citep{Frank:96}, and German \citep{dipper03, rohrerForst2006}. They developed aligned f-structure analyses for a tractor manual which existed as an aligned corpus in all three languages. Even with three closely related languages, it was clear that full f-structure alignment was not possible \citep{Buttetal:99} due to fundamental syntactic differences in the languages. Later, the Fuji Xerox Corporate Research Group and the University of Bergen joined the initiative with a Japanese \citep{Masuichi2003JapanesePO} and a Norwegian grammar \citep{dyvikEtAl2016,dyvikEtAl2019} respectively. Other longer-term academic efforts participating in ParGram concern the development of Urdu \citep{buttking02,buttking07} and Polish \citep{patprz12} LFG implementations. Finally, further ParGram efforts have given rise to computational LFGs for Arabic \citep{attia2006, attia12}, Chinese \citep{Fang-King-GEAF07}, Danish \citep{orsnes2006}, Georgian \citep{meurer09}, Hungarian \citep{laczkorakosi08}, Indonesian \citep{arkaetal09,arka2012}, Korean \citep{kimetal03}, Malagasy \citep{malagasy2006}, Tamil \citep{tamil19}, Tigrinya \citep{amlesomkinfe2011}, Turkish \citep{cetinogluoflazer18}, Welsh \citep{mittendorfsadler06}, and Wolof \citep{Dione:Disambiguation}.

The project resulted in the creation of  LFG grammars in these multiple languages and hence a greater understanding of the parallelism (or lack thereof) for the LFG analyses of particular constructions. Major issues in LFG analysis and architecture highlighted by the ParGram project included: Copular constructions and in particular whether there is a copular {\em be} predicate and whether the predicated argument has a subject (\textsc{xcomp}-like) or not (\textsc{predlink}) \citep{dalrympleetal04copular,attia08}; how to handle argument-changing relations such as the passive, causative, benefactives, complex predicates, and interactions thereof, including morphological and syntactic interactions (\cite{boegelbuttking2019}; see \sectref{sec:theoretical_implications}); whether auxiliaries have predicates or just supply tense and aspect features to the f-structure \citep{buttetal96,dyvik99}; the interaction of tokenization and morphology with the c- and f-structures, especially around features like Welsh mutations \citep{mittendorfsadler06} and Urdu complex predicates \citep{boegelbuttking2019}.
In addition, the ParGram project resulted in improvements to the grammar development platform (\sectref{sec:corecomponents}, \sectref{sec:devtools} and \sectref{sec:runtime_performance}) and in best practices for distributed parallel grammar development.


In addition to the traditional LFG-style ParGram grammars which use annotated phrase structure rules to create the c- and f-structure representations, the ParGram project also includes several automatically induced grammars that create ParGram compatible f-structures, i.e.\ f-structures using the same feature space as the grammars described above, but which are learned from tree and f-structure banks \citep{cahilletal02}. These grammars are robust in that they produce f-structures for nearly any sentence, at the cost of producing structures which sometimes violate core LFG principles such as completeness and coherence. See \sectref{sec:applications} for applications which require such robustness.

An influential initiative that resembles ParGram is the Universal Dependencies (UD) initiative (\cite{universaldep13}; see also \citetv{chapters/Dependency}). Like ParGram, it aims at parallel representations across languages, and  UD follows LFG concerning many of the distinctions made at the level of syntactic dependencies and grammatical functions respectively \citep{USD2014}. This being said, surface-oriented dependency structures as used in UD cannot be as parallel as the  more abstract f-structures of ParGram. \cite{korsak18} and \cite{prz:pat:19:lre} discuss the similarities between LFG and UD and investigate mapping between LFG f-structures and UD. Another noteworthy difference between ParGram and UD is that ParGram has been developing reversible XLE grammars whereas UD focuses solely on parsing.


\section{Applications}
\label{sec:applications}

Some applications integrating natural language processing  only require parsing. For  these applications, parsing should be robust to typos and grammatical errors, unusual constructions, unknown words, etc. In addition, minor issues in parsing may be unimportant for these applications because systematic errors can be compensated for within the system. Semantic search is  an application that requires only parsing, needs to be robust, and can tolerate certain parsing errors.

Other applications, e.g.\ sentence condensation, transfer-based machine translation (MT) and conversational agents, require both parsing and generation. Applications using generation generally require highly grammatical output since users are sensitive to malformed natural language such as incorrect subject-verb agreement. Since corpus-induced grammars do not lend themselves to refinement in order to control generation, hand-crafted grammar implementations such as LFG grammars are still the means of choice for the generation of high-quality text.

Finally, there are applications that require grammaticality judgments. This is the case of grammar checkers, both general-purpose ones and grammar checkers for computer-assisted language learning (CALL). Parsers trained on general-purpose treebanks cannot be used for this purpose, so these applications are another natural fit for hand-crafted grammar implementations. In our opinion, LFG suits this purpose particularly well because its terminology is relatively close to that used in language instruction.

%\input{robustness}

\subsection{Applications requiring deep features and robustness}
        
For applications that require mainly natural language understanding, parsing needs to be robust to unexpected words %errors and unknown words 
and constructions. To provide the robustness necessary for these applications, domain-specific grammars can be created based on a general large-scale grammar \citep{kimetal03,kingmaxwell07}. However, this is often not enough to cover all use cases. LFG grammars can use morphological guessers to cover unknown vocabulary \citep{dost-king-2009-using,boegelbuttking2019}, can parse fragments of the structure, e.g.\ provide f-structures for all the noun phrases even if they cannot be formed into a sentence \citep{riezleretal03}, and can include fall-back rules (mal-rules \cite{schneider-mccoy-1998-recognizing-syntactic,reuer03,khader03,fortmannforst04,Ben:Fli:Oep:Wal:Bal:04}) explicitly accounting for certain types of ungrammaticality, e.g.\ incorrect subject-verb agreement. 

Semantic search is one application which benefits from the deep LFG representations. As a search application, the goal is to find documents which are relevant to the query and, ideally, to highlight the passage in the document most relevant to the query. Semantic search moves beyond keyword matching to match the relationships between entities in the query. It can include queries that are full interrogatives as well as ones that are  phrases. The ParGram XLE English grammar was used in the Powerset Inc.\ semantic search engine for searching Wikipedia articles. By using LFG representations for the query and the documents it can differentiate between {\em who acquired PeopleSoft} and {\em who did PeopleSoft acquire}, where PeopleSoft is the object in the first  question and the subject in the second. By using a fragment grammar as a backup, longer sentences could be partially parsed, e.g.\ the first conjunct of a coordinated sentence could be parsed even if the second failed. This combined with the redundancy across the articles made using an LFG grammar feasible for moving beyond keyword search. The f-structures were mapped to abstract knowledge representations which went beyond grammatical functions to semantic rules, e.g.\ mapping {\em Oracle acquired PeopleSoft} and {\em PeopleSoft was acquired by Oracle} and even {\em Oracle's acquisition of PeopleSoft} to the same abstract representation.

A more complex application than semantic search is question answering. Unlike search, question answering uses a document collection to find the answer to the query, which is generally in the form of a natural-language question, and present it to the user. The PARC Bridge system \citep{bobrowetal07} used the XLE ParGram English grammar as its base and mapped the query and documents to an abstract knowledge representation using ordered rewrite rules, deep lexical resources such as WordNet \citep{wordnet} and VerbNet \citep{kipperetal2000,levin93}, and  knowledge resources such as Cyc \citep{lenat95}. The queries and documents were then matched against one another with a graph-based algorithm. An interesting extension of this was to perform entailment and contradiction detection (ECD)  \citep{bobrowetal07} with a graph-based module that determined whether one sentence entailed or contradicted (or neither) the other. ECD depended on understanding the roles between the entities as determined by the LFG grammar as well as detailed lexical knowledge.

\cite{burton06} describes a tutorial system which uses the XLE English grammar for its language-understanding component. The tutorial system is provided by Acuitus and  teaches network administration. The coursework includes a set of troubleshooting exercises where students find and fix problems. During these exercises the computer helps the students  when they ask for help or based on their actions. The system asks the student a mix of multiple-choice, short-answer, and natural-language questions. The idea behind using natural-language interactions is to encourage students to think beyond what multiple-choice questions provide and to allow more complex questions and answers. The system converts the f-structures from the student  input  to semantic interpretations via the transfer rule system \citep{crouch06}. Both the syntactic parsing and the semantics are adapted to the domain to provide more accurate and robust results.

Historically, hand-crafted LFG implementations have had a hard time competing with machine-learned constituency or dependency parsers  in terms of robustness, i.e.\ providing a parse for all input, and speed for purely understanding-oriented applications, even though they are often superior  in terms of systematicity and detail of analysis and despite the fact that machine-learned parsers often produce illogical parses for input where LFG grammars would fail to produce a parse. Because of this speed and perceived robustness, machine-learning-based dependency parsers 
have become increasingly popular, as is evident from the shared tasks of the Conference on Computational Natural Language Learning (CoNLL) series. Interesting though, the CoNLL tasks now often integrate UD representations \citep{universaldep13}, which can be seen as less fine-grained f-structures (see \sectref{sec:pargram} for more details on UD). The combination of hand-crafted grammars, fall-back techniques, and statistical parser selection as described in this chapter allow LFG and other rule-based grammars to be used in applications requiring robustness (see also \cite{ivanovaetal16}).


% \input{grammaticality}
\subsection{Applications requiring grammaticality}

%Other applications, e.g. sentence condensation and transfer-based machine translation (MT), require both parsing and generation. 

Certain applications  not only aim to map text to representations more amenable to the computation of meaning, but they also take abstract meaning representations, including f-structures, as input and map them to text. Among such applications are sentence condensation and transfer-based machine translation, both applications for which LFG implementations have been used because f-structures are abstract enough to facilitate transformations like the removal of certain adjuncts or the transfer from a source to a target language. Furthermore, since corpus-induced grammars do not lend themselves to refinement in order to control generation, hand-crafted grammar implementations are still the means of choice for the generation of high-quality  text.

Sentence condensation is a form of summarization \citep{knightmarcu2000,jing2000}. It takes a long sentence and produces a shorter sentence which preserves the core meaning of the original sentence. This requires the ability to identify the core part of the original sentence and to generate a grammatical shorter sentence. \cite{riezleretal03} and \cite{crouchetal04} used the ParGram XLE grammar to create a sentence condensation system for English. The LFG f-structure was used to identify the core meaning, e.g.\ by removing adjuncts other than negation. A new f-structure was created which contained only this core meaning. This new f-structure was then run through the grammar in the generation direction to generate the shorter, condensed sentence. This sentence was guaranteed to be grammatical since it met the well-formedness conditions of the grammar. Since multiple strings (e.g.\ sentences) can map to the same f-structure, more than one condensed sentence can often be generated from a single f-structure. This can be partially controlled by Optimality-Theory marks in the grammar in XLE \citep{franketal98}. The choice between the remaining sentences can be done with a language model \citep{riezleretal03}. A related application is note taking where longer texts are condensed into legible notes \citep{kaplanetal2005notetaking}.

Machine translation (MT) involves automatically translating a text from one language (the source) to another (the target). The resulting translation has to preserve the meaning and to be grammatical. LFG f-structures have been used for MT \citep{Oep:Dyv:Lon:04,riezler-maxwell-iii-2006-grammatical,avramidiskuhn09,grahametal09,graham12,grahamvangenabith12,homolacoler12}. The idea  is that the f-structure encodes the meaning of the sentence more abstractly than the surface form of the text and so can be used as the level for translation. That is, f-structure enables translation by transfer across structures and not just an interlingua across words \citep{kaplanetal1989}. In theory, simply substituting the \textsc{pred} values in the f-structure could produce an f-structure in the target language and the LFG grammar can then be used to generate the translation. In practice, f-structures still encode enough language-specific syntactic information that additional transfer rules need to be applied before the generation step. For example, one language may use indefinite singular determiners (e.g.\ English {\em a}) while the other may not, in which case the determiner would have to be deleted (in the source language) or inserted (in the target language). The LOGON MT project \citep{Oep:Dyv:Lon:04} provides an interesting approach with parsing via the LFG Norwegian NorGram grammar, transfer to semantic MRS \citep{copestakeetal05} and generation via an HPSG English grammar.
Although LFG-based MT systems can be brittle since there has to be a successful parse, transfer, and generation, when a translation is produced it is generally of high quality both in terms of preserving the meaning and of being grammatically well-formed.

Consider the English and German sentences in (\ref{ex:across-the-city}) and (\ref{ex:in-der-ganzen-stadt}), for which the corresponding f-structures are displayed in Figure~\ref{fig:mt}.

\ea
   \label{ex:across-the-city} Across the city, monuments to prosperity have sprung up.
    \z

\ea
 \label{ex:in-der-ganzen-stadt}
 \gll   In der ganzen Stadt sind Denkm{\"a}ler des Wohlstands entstanden.\\
 in the whole city be monuments of.the prosperity up.spring\\
 \glt Across the city, monuments to prosperity have sprung up.
\z

\noindent Apart from the fact that the German analysis of adjunct NPs in the genitive is not parallel to other ParGram implementations and that the German finite-state morphology decomposes the word {\em Wohlstand}, which gives rise to a \textsc{mod} dependency under the \textsc{Subj Adj-Gen}, the f-structures are surprisingly parallel. (At first sight, this is obscured by the fact that in the German f-structure, the sub-f-structures under \textsc{Topic} and in the \textsc{Adjunct} set are the same.) Even though the English sentence is headed by a particle verb while the German one is not, there is a single \textsc{pred} value for the head verb on either side; even though the subject of the English sentence precedes the verb while the one of the German sentence follows the verb, both appear in the respective f-structure under \textsc{subj};  even though the auxiliary in the English sentence is  {\em have} while the German verb {\em entstehen} requires the auxiliary {\em sein} (`to be') for perfect tenses, the auxiliaries contribute the same value for \textsc{tns-asp perf}. As a result, the transfer component can concentrate on  word-to-word translation equivalencies while letting the language-specific grammars take care of well-formedness conditions independent of the language pair under consideration. An example of a non-trivial translation equivalency is the one between {\em across the city} and {\em in der ganzen Stadt} (literally `in the entire city'), as the English phrase might also correspond to {\em durch die Stadt} (literally `through the city') in other contexts (especially in combination with motion verbs).

\begin{figure}[ht]
\caption{F-structures for English and German translation equivalents\label{fig:mt}}
\includegraphics[width=0.5\textwidth]{figures/EnglishFStr.png}\includegraphics[width=0.5\textwidth]{figures/GermanFStr.png}
\end{figure}


Certain other applications do not require semantic representations or grammatical text output but do require the system to have a notion of grammaticality as their purpose is to highlight ungrammatical (or otherwise undesired) passages in text. Such systems can  be directed to a general public of people producing texts  or explicitly target second-language learners, sometimes even second-language learners with a specific first-language background. The latter application, in the context of Intelligent Computer-Assisted Language Learning (ICALL), has used LFG implementations, typically augmented with  mal-rules \citep{rypaetal95,reuer03,khader03,fortmannforst04}. Mal-rules are rules or rule extensions that cover ungrammatical constructions typically produced by second-language learners, e.g.\ NPs where determiners or adjectives do not agree with the head noun, NPs with countable head nouns in the singular that are not preceded by a determiner, or sentences with an ungrammatical order of constituents or a violation of subject-verb agreement. As typical mistakes made by second-language learners depend significantly on their native language as well as on other languages they know, mal-rules can be optimized with respect to their coverage  more easily when the linguistic background of the audience is known. A machine-learning-based approach to ICALL exploiting features provided by the English ParGram LFG implementation is described by \cite{berendetal13}.

A final application we discuss is
natural language understanding (NLU) components  used in   car computers or in personal assistants on mobile devices. Those
% NLU
components often combine grammar-based analysis and deep-learn\-ing-based neural networks or statistical models learned from annotated data. Moreover, machine-learning-based
NLU models depend on large amounts of training data from the relevant domain. Since such data is hard to collect and costly to annotate, much of it is generated by means of grammars. For the most part, the grammars used to this end are simple, largely context-free grammars. However, as the semantic representations used for NLU become increasingly sophisticated, the use of more powerful grammar formalisms such as LFG can be used for the generation of high-quality grammatical training data. 


        

%\input{mappings}

\section{Conclusion}
This chapter provided an overview of computational implementations of LFG. LFG was designed from the outset to be computationally tractable and has a strong history of broad-coverage implementations for multiple languages, primarily through the ParGram project  which is built on the XLE grammar development platform. As with theoretical LFG, implemented grammars primarily focus on c-structure and f-structure, but extensive work has been done on using the resulting f-structures as input to semantics and abstract knowledge representation, and some work has focused on the integration of morphological and phonological information as well as argument structure. The ParGram project is based on the theoretical LFG hypothesis that languages are more similar at f-structure, which encodes grammatical functions, than at c-structure. This f-structure similarity can then be exploited in applications such as machine translation. Other applications which take advantage of the more abstract f-structures and the ability of LFG grammars to parse and generate as well as to detect (un)grammaticality include computer-assisted language learning, question answering, and sentence condensation. From a theoretical linguistic perspective, implemented grammars allow the linguist to test analyses and to see interactions between different parts of the grammar.

%\nocite{*}

\section{Acknowledgements}

We would like to thank Emily Bender, Gerlof Bouma, Ron Kaplan, Agnieszka Patejuk, Victoria Ros\'en, Annie Zaenen, and two anonymous reviewers for detailed comments on this chapter. All remaining errors are our own.

\sloppy
\printbibliography[heading=subbibliography,notkeyword=this]
\end{document}
